{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gpflow\n",
    "import tensorflow as tf\n",
    "from gpflow.kernels import SquaredExponential as SE\n",
    "from gpflow.kernels import RationalQuadratic as RQ\n",
    "from gpflow.kernels import Linear, Periodic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from gpflow.ci_utils import ci_niter\n",
    "sys.path.append('../code')\n",
    "from gplar_pascal import GPLARmodel\n",
    "from gpar.regression import GPARRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_data(kernels, N_train, N_test, noise, xmin=0, xmax=1):\n",
    "    x = np.linspace(xmin, xmax, 1000).reshape(-1,1)\n",
    "    idx = np.random.choice(1000,N_train+N_test,replace=False)\n",
    "    idx_train, idx_test = idx[:N_train], idx[N_train:]\n",
    "    inputx = x\n",
    "    output, test, true = [],[],[]\n",
    "    plt.figure(figsize=(20, len(kernels)*3))\n",
    "    for i in range(len(kernels)):\n",
    "        Kx = kernels[i](inputx)\n",
    "        h = np.random.multivariate_normal(np.zeros(1000), Kx)[:,None]\n",
    "        hh = h[idx_train]\n",
    "        hh_test = h[idx_test]\n",
    "        y = hh + np.random.normal(0,noise[i],N_train).reshape(-1,1)\n",
    "        y_test = hh_test + np.random.normal(0,noise[i],N_test).reshape(-1,1)\n",
    "        plt.subplot(len(kernels),1, i+1)\n",
    "        plt.scatter(x[idx_train], y, c='black', label='Train', s=10)\n",
    "        plt.scatter(x[idx_test], y_test, c='red', label='Test', s=10)\n",
    "        plt.plot(x, h)\n",
    "        plt.ylabel('output '+str(i))\n",
    "        \n",
    "        inputx = np.concatenate((inputx, h), axis=1)\n",
    "        output.append(y)\n",
    "        true.append(h)\n",
    "        test.append(y_test)\n",
    "    return x[idx_train], np.stack(output,axis=1)[:,:,0], \\\n",
    "            x, np.stack(true,axis=1)[:,:,0],\\\n",
    "            x[idx_test], np.stack(test, axis=1)[:,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "kernels_se = [SE(active_dims=[0],variance=1,lengthscales=0.05),\n",
    "           SE(active_dims=[0],variance=1,lengthscales=0.02)\n",
    "               +SE(active_dims=[1],variance=5,lengthscales=2.),\n",
    "           SE(active_dims=[0],variance=1,lengthscales=0.05)\n",
    "               +SE(active_dims=[1,2],variance=5,lengthscales=[3.,2.]),\n",
    "           SE(active_dims=[0],variance=0.3,lengthscales=0.1)\n",
    "               +SE(active_dims=[1,2,3],variance=1.,lengthscales=[2.,3.,3.])]\n",
    "\n",
    "\n",
    "x_train, y_train, x_, h_, x_test, y_test = generate_synthetic_data(kernels_se, 100, 100, [0.1,0.1,0.1,0.1,0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def meanstd(y):\n",
    "    mean, std = [],[]\n",
    "    for i in range(y.shape[1]):\n",
    "        available = ~np.isnan(y[:,i])\n",
    "        y_i = y[available,i]\n",
    "        mean.append(np.mean(y_i))\n",
    "        std.append(np.std(y_i))\n",
    "    return np.stack(mean), np.stack(std)\n",
    "\n",
    "y_mean, y_std = meanstd(y_train)\n",
    "y_train_norm = (y_train-mean)/std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('345_all.csv', dtype=np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eeg_train_2 = df[['FZ','F1','F2','F3','F4','F5','F6']].copy()\n",
    "eeg_train_2['FZ'].iloc[175:200] = None\n",
    "eeg_train_2['F1'].iloc[175:200] = None\n",
    "eeg_train_2['F2'].iloc[175:200] = None\n",
    "eeg_train_2['F3'].iloc[50:75] = None\n",
    "eeg_train_2['F4'].iloc[50:75] = None\n",
    "eeg_train_2['F5'].iloc[100:125] = None\n",
    "eeg_train_2['F6'].iloc[100:125] = None\n",
    "\n",
    "eeg_test_2 = df[['time','F3','F4','F5','F6','FZ','F1','F2']].iloc[50:200].copy()\n",
    "eeg_test_2['FZ'].iloc[:125] = None\n",
    "eeg_test_2['F1'].iloc[:125] = None\n",
    "eeg_test_2['F2'].iloc[:125] = None\n",
    "eeg_test_2['F3'].iloc[25:] = None\n",
    "eeg_test_2['F4'].iloc[25:] = None\n",
    "eeg_test_2['F5'].iloc[:50] = None\n",
    "eeg_test_2['F6'].iloc[:50] = None\n",
    "eeg_test_2['F5'].iloc[75:] = None\n",
    "eeg_test_2['F6'].iloc[75:] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_eeg = np.array(df['time']).reshape(-1,1)\n",
    "y_eeg = np.array(eeg_train_2)\n",
    "means_eeg, stds_eeg = [],[]\n",
    "for i, name in enumerate(eeg_train_2.columns):\n",
    "    available = ~np.isnan(y_eeg[:,i])\n",
    "    y_i = y_eeg[available, i]\n",
    "    means_eeg.append(np.mean(y_i))\n",
    "    stds_eeg.append(np.std(y_i))\n",
    "means_eeg, stds_eeg = np.stack(means_eeg), np.stack(stds_eeg)\n",
    "\n",
    "def normalise_y(y_, means, stds): return (y_ - means)/stds\n",
    "def unnormalise_y(y_, means, stds): return y_*stds + means\n",
    "\n",
    "y_eeg_normalise = normalise_y(y_eeg ,means_eeg, stds_eeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('exchange_rates_2000-2010.csv', dtype=np.float64)\n",
    "exchange = df[df.year<2008]\n",
    "exchange = exchange[exchange.year>=2005]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exchange_train_1 = exchange[[ 'USD/EUR', 'USD/GBP', 'USD/HKD', 'USD/KRW', 'USD/CAD', 'USD/JPY','USD/AUD']].copy()\n",
    "exchange_train_1['USD/EUR'].iloc[:50]=None\n",
    "exchange_train_1['USD/GBP'].iloc[:50]=None\n",
    "exchange_train_1['USD/JPY'].iloc[500:550]=None\n",
    "exchange_train_1['USD/AUD'].iloc[500:550]=None\n",
    "\n",
    "exchange_test_1 = exchange[['year','USD/EUR', 'USD/GBP','USD/JPY','USD/AUD']].iloc[:550].copy()\n",
    "exchange_test_1['USD/EUR'].iloc[50:] = None\n",
    "exchange_test_1['USD/GBP'].iloc[50:] = None\n",
    "exchange_test_1['USD/JPY'].iloc[:500] = None\n",
    "exchange_test_1['USD/AUD'].iloc[:500] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ex = np.array(exchange['year']).reshape(-1, 1)\n",
    "y_ex = np.array(exchange_train_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_ex.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means_exchange, stds_exchange = [],[]\n",
    "for i in range(y_ex.shape[1]):\n",
    "    available = ~np.isnan(y_ex[:,i])\n",
    "    y_i = y_ex[available, i]\n",
    "    means_exchange.append(np.mean(y_i))\n",
    "    stds_exchange.append(np.std(y_i))\n",
    "\n",
    "means_exchange, stds_exchange = np.stack(means_exchange), np.stack(stds_exchange)\n",
    "def normalise_y(y_, means, stds): return (y_ - means)/stds\n",
    "def unnormalise_y(y_, means, stds): return y_*stds + means\n",
    "y_ex_normalise = normalise_y(y_ex, means_exchange, stds_exchange)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "df = pd.read_excel('data.xlsx', dtype=np.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The measurements are made every 5 minutes and spans the entire month July of 2013\n",
    "July 31 days, every day 24 hours, every hour 60 = 5*12 minitues\n",
    "31*24*12 = 8928\n",
    "if 1 means 1 day => total will be 31 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['time'] = np.linspace(0,31,8928)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Bra height</th>\n",
       "      <th>Sot height</th>\n",
       "      <th>Cam height</th>\n",
       "      <th>Chi height</th>\n",
       "      <th>Bra speed</th>\n",
       "      <th>Sot speed</th>\n",
       "      <th>Cam speed</th>\n",
       "      <th>Chi speed</th>\n",
       "      <th>Bra temp</th>\n",
       "      <th>Sot temp</th>\n",
       "      <th>Cam temp</th>\n",
       "      <th>Chi temp</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.67</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.46</td>\n",
       "      <td>1.60</td>\n",
       "      <td>7.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.7</td>\n",
       "      <td>11.1</td>\n",
       "      <td>15.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.1</td>\n",
       "      <td>14.9</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.69</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.49</td>\n",
       "      <td>1.63</td>\n",
       "      <td>7.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.9</td>\n",
       "      <td>10.8</td>\n",
       "      <td>15.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.1</td>\n",
       "      <td>14.9</td>\n",
       "      <td>0.003473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.72</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.51</td>\n",
       "      <td>1.65</td>\n",
       "      <td>7.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.2</td>\n",
       "      <td>10.2</td>\n",
       "      <td>15.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.006945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.74</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.54</td>\n",
       "      <td>1.67</td>\n",
       "      <td>7.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.4</td>\n",
       "      <td>10.7</td>\n",
       "      <td>15.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.010418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.75</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.57</td>\n",
       "      <td>1.70</td>\n",
       "      <td>7.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.9</td>\n",
       "      <td>11.7</td>\n",
       "      <td>15.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.013890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8923</th>\n",
       "      <td>2.63</td>\n",
       "      <td>2.29</td>\n",
       "      <td>2.13</td>\n",
       "      <td>2.24</td>\n",
       "      <td>4.7</td>\n",
       "      <td>8.2</td>\n",
       "      <td>2.4</td>\n",
       "      <td>8.4</td>\n",
       "      <td>18.1</td>\n",
       "      <td>18.5</td>\n",
       "      <td>18.7</td>\n",
       "      <td>18.6</td>\n",
       "      <td>30.986110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8924</th>\n",
       "      <td>2.61</td>\n",
       "      <td>2.26</td>\n",
       "      <td>2.09</td>\n",
       "      <td>2.22</td>\n",
       "      <td>4.5</td>\n",
       "      <td>8.6</td>\n",
       "      <td>2.2</td>\n",
       "      <td>8.2</td>\n",
       "      <td>18.3</td>\n",
       "      <td>18.5</td>\n",
       "      <td>18.6</td>\n",
       "      <td>18.5</td>\n",
       "      <td>30.989582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8925</th>\n",
       "      <td>2.57</td>\n",
       "      <td>2.21</td>\n",
       "      <td>2.05</td>\n",
       "      <td>2.19</td>\n",
       "      <td>4.3</td>\n",
       "      <td>8.3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.8</td>\n",
       "      <td>18.2</td>\n",
       "      <td>18.5</td>\n",
       "      <td>18.6</td>\n",
       "      <td>18.4</td>\n",
       "      <td>30.993055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8926</th>\n",
       "      <td>2.55</td>\n",
       "      <td>2.19</td>\n",
       "      <td>2.01</td>\n",
       "      <td>2.16</td>\n",
       "      <td>4.8</td>\n",
       "      <td>6.6</td>\n",
       "      <td>2.8</td>\n",
       "      <td>7.7</td>\n",
       "      <td>18.2</td>\n",
       "      <td>18.5</td>\n",
       "      <td>18.6</td>\n",
       "      <td>18.4</td>\n",
       "      <td>30.996527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8927</th>\n",
       "      <td>2.53</td>\n",
       "      <td>2.17</td>\n",
       "      <td>1.98</td>\n",
       "      <td>2.14</td>\n",
       "      <td>5.2</td>\n",
       "      <td>5.9</td>\n",
       "      <td>2.6</td>\n",
       "      <td>6.5</td>\n",
       "      <td>18.1</td>\n",
       "      <td>18.5</td>\n",
       "      <td>18.5</td>\n",
       "      <td>18.5</td>\n",
       "      <td>31.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8928 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Bra height  Sot height  Cam height  Chi height  Bra speed  Sot speed  \\\n",
       "0           2.67         NaN        1.46        1.60        7.3        NaN   \n",
       "1           2.69         NaN        1.49        1.63        7.2        NaN   \n",
       "2           2.72         NaN        1.51        1.65        7.3        NaN   \n",
       "3           2.74         NaN        1.54        1.67        7.2        NaN   \n",
       "4           2.75         NaN        1.57        1.70        7.4        NaN   \n",
       "...          ...         ...         ...         ...        ...        ...   \n",
       "8923        2.63        2.29        2.13        2.24        4.7        8.2   \n",
       "8924        2.61        2.26        2.09        2.22        4.5        8.6   \n",
       "8925        2.57        2.21        2.05        2.19        4.3        8.3   \n",
       "8926        2.55        2.19        2.01        2.16        4.8        6.6   \n",
       "8927        2.53        2.17        1.98        2.14        5.2        5.9   \n",
       "\n",
       "      Cam speed  Chi speed  Bra temp  Sot temp  Cam temp  Chi temp       time  \n",
       "0           6.7       11.1      15.2       NaN      15.1      14.9   0.000000  \n",
       "1           6.9       10.8      15.2       NaN      15.1      14.9   0.003473  \n",
       "2           7.2       10.2      15.2       NaN      15.0      15.0   0.006945  \n",
       "3           7.4       10.7      15.1       NaN      15.0      15.0   0.010418  \n",
       "4           6.9       11.7      15.1       NaN      15.0      15.0   0.013890  \n",
       "...         ...        ...       ...       ...       ...       ...        ...  \n",
       "8923        2.4        8.4      18.1      18.5      18.7      18.6  30.986110  \n",
       "8924        2.2        8.2      18.3      18.5      18.6      18.5  30.989582  \n",
       "8925        2.0        7.8      18.2      18.5      18.6      18.4  30.993055  \n",
       "8926        2.8        7.7      18.2      18.5      18.6      18.4  30.996527  \n",
       "8927        2.6        6.5      18.1      18.5      18.5      18.5  31.000000  \n",
       "\n",
       "[8928 rows x 13 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_train = df[['Cam temp','Chi temp', 'Bra height','Sot height','Cam height','Chi height','Bra speed','Sot speed','Cam speed','Chi speed','Bra temp','Sot temp']].copy()\n",
    "weather_train['Cam temp'].iloc[3456:4608]=None #12-16 4days\n",
    "weather_train['Chi temp'].iloc[4032:5184]=None #14-18 4days\n",
    "weather_train['Bra temp'].iloc[6912:8064]=None #24-28 4days\n",
    "weather_train['Sot temp'].iloc[7488:8640]=None #26-30 4days\n",
    "\n",
    "weather_test = df[['time','Cam temp','Chi temp', 'Bra temp','Sot temp']].iloc[3456:8640].copy()\n",
    "weather_test['Cam temp'].iloc[1152:] = None\n",
    "weather_test['Chi temp'].iloc[:576] = None\n",
    "weather_test['Chi temp'].iloc[1728:] = None\n",
    "weather_test['Bra temp'].iloc[:3456] = None\n",
    "weather_test['Bra temp'].iloc[4608:] = None\n",
    "weather_test['Sot temp'].iloc[:4032] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_wt = np.array(df['time']).reshape(-1, 1)\n",
    "y_wt = np.array(weather_train)\n",
    "means_weather, stds_weather = [],[]\n",
    "for i in range(y_wt.shape[1]):\n",
    "    available = ~np.isnan(y_wt[:,i])\n",
    "    y_i = y_wt[available, i]\n",
    "    means_weather.append(np.mean(y_i))\n",
    "    stds_weather.append(np.std(y_i))\n",
    "\n",
    "means_weather, stds_weather = np.stack(means_weather), np.stack(stds_weather)\n",
    "def normalise_y(y_, means, stds): return (y_ - means)/stds\n",
    "def unnormalise_y(y_, means, stds): return y_*stds + means\n",
    "y_wt_normalise = normalise_y(y_wt, means_weather, stds_weather)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpar = GPARRegressor(scale=0.02,\n",
    "                    linear=False, nonlinear=True, nonlinear_scale=1.0,\n",
    "                    noise=0.01,\n",
    "                    impute=True, replace=True, normalise_y=False)\n",
    "gpar_back = GPARRegressor(scale=0.02,\n",
    "                    linear=False, nonlinear=True, nonlinear_scale=1.0,\n",
    "                    noise=0.01,\n",
    "                    impute=True, replace=True, normalise_y=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ind = np.linspace(x_wt.min(), x_wt.max(), 300)\n",
    "weather_gpar = GPARRegressor(scale=0.2,\n",
    "                          linear=True, linear_scale=10.,\n",
    "                          nonlinear=True, nonlinear_scale=1.,\n",
    "                          noise=0.1,\n",
    "                          impute=True, replace=True, normalise_y=True,\n",
    "                          x_ind=x_ind)\n",
    "weather_gpar_back = GPARRegressor(scale=0.2,\n",
    "                          linear=True, linear_scale=10.,\n",
    "                          nonlinear=True, nonlinear_scale=1.,\n",
    "                          noise=0.1,\n",
    "                          impute=True, replace=True, normalise_y=True,\n",
    "                          x_ind=x_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_gpar_back = GPARRegressor(scale=0.2,\n",
    "                          linear=True, linear_scale=10.,\n",
    "                          nonlinear=True, nonlinear_scale=1.,\n",
    "                          noise=0.1,\n",
    "                          impute=True, replace=True, normalise_y=True,\n",
    "                          x_ind=x_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_gpar.fit(x_wt, y_wt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_wt_back = y_wt[:,::-1].copy()\n",
    "weather_gpar_back.fit(x_wt, y_wt_back)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backward_samples = weather_gpar_back.sample(x_wt[3456:5000,0], num_samples=50, latent=True, posterior=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backward_samples_2 = weather_gpar_back.sample(x_wt[5000:7000,0], num_samples=50, latent=True, posterior=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backward_samples_3 = weather_gpar_back.sample(x_wt[7000:8640,0], num_samples=50, latent=True, posterior=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backward_samples_ = np.concatenate([backward_samples, backward_samples_2,backward_samples_3], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backward_samples_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z1 = np.linspace(np.min(x_wt),np.max(x_wt),300).reshape(300,1)\n",
    "z2 = np.linspace(np.min(x_wt),np.max(x_wt),300).reshape(300,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from lab.torch import B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.linspace(np.min(x_wt),np.max(x_wt),300).reshape(300,1)\n",
    "samples = weather_gpar.sample(z, num_samples=100, latent=True, posterior=True)\n",
    "means, std = np.mean(samples,axis=0), np.std(samples,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.linspace(np.min(x_wt),np.max(x_wt),300).reshape(300,1)\n",
    "samples = weather_gpar_back.sample(z, num_samples=100, latent=True, posterior=True)\n",
    "means, std = np.mean(samples,axis=0), np.std(samples,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('backwards_gpar.npy', 'wb') as f:\n",
    "    np.save(f, means)\n",
    "    np.save(f, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('forward_gpar.npy', 'rb') as f:\n",
    "    a = np.load(f)\n",
    "    b = np.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('backwards_gpar.npy', 'rb') as f:\n",
    "    c = np.load(f)\n",
    "    d = np.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = unnormalise_y(c, means_weather[::-1], stds_weather[::-1])\n",
    "d = unnormalise_y(d, 0, stds_weather[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, y_wt.shape[1]*5))\n",
    "x = x_wt[3456:8640,0]\n",
    "mean, lower, upper = np.mean(forward_samples_, axis=0), np.percentile(forward_samples_, 2.5, axis=0), np.percentile(forward_samples_, 100-2.5, axis=0)\n",
    "for i, name in enumerate(weather_test.columns[1:]):\n",
    "    p = list(weather_train.columns).index(name)\n",
    "    plt.subplot(y_wt.shape[1],1,i+1)\n",
    "\n",
    "    \n",
    "    plt.plot(x, mean[:,p], c='red', label = 'forward')\n",
    "    plt.fill_between(x, lower[:,p], upper[:,p], facecolor='tab:red', alpha=.25)\n",
    "    \n",
    "    plt.scatter(x, y_wt[3456:8640, p], c='black', s=15, marker='+')\n",
    "    plt.scatter(weather_test['time'], weather_test[name], c='green', s=20)\n",
    "    plt.xlabel('Time (second)')\n",
    "    plt.ylabel(name)\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, y_wt.shape[1]*5))\n",
    "x = x_wt[3456:8640,0]\n",
    "mean, lower, upper = np.mean(backward_samples_, axis=0), np.percentile(backward_samples_, 2.5, axis=0), np.percentile(backward_samples_, 100-2.5, axis=0)\n",
    "for i, name in enumerate(weather_test.columns[1:]):\n",
    "    p = list(weather_train.columns).index(name)\n",
    "    plt.subplot(y_wt.shape[1],1,i+1)\n",
    "\n",
    "    \n",
    "    plt.plot(x, mean[:,11-p], c='red', label = 'forward')\n",
    "    plt.fill_between(x, lower[:,11-p], upper[:,11-p], facecolor='tab:red', alpha=.25)\n",
    "    \n",
    "    plt.scatter(x, y_wt[3456:8640, p], c='black', s=15, marker='+')\n",
    "    plt.scatter(weather_test['time'], weather_test[name], c='green', s=20)\n",
    "    plt.xlabel('Time (second)')\n",
    "    plt.ylabel(name)\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, y_wt.shape[1]*5))\n",
    "x = x_wt[3456:8640,0]\n",
    "for i, name in enumerate(weather_test.columns[1:]):\n",
    "    p = list(weather_train.columns).index(name)\n",
    "    plt.subplot(y_wt.shape[1],1,i+1)\n",
    "\n",
    "    \n",
    "    mean = np.mean((Hmeans[p]+BHmeans[p])/2.0,axis=0)\n",
    "    var = np.mean(((Hmeans[p]+BHmeans[p])/2.0)**2 + (Hvars[p]+BHvars[p])/4.0 + gplar_weather.likelihoods[p].variance,axis=0) - mean**2\n",
    "    mean, var = mean[:,0],var[:,0]\n",
    "    mean, std = mean* stds_weather[p] + means_weather[p], stds_weather[p] * np.sqrt(var)\n",
    "    \n",
    "    plt.plot(x, mean, c='red', label = 'GPLAR')\n",
    "    plt.fill_between(x, mean-1.96*std, mean+1.96*std, facecolor='tab:red', alpha=.25)\n",
    "    \n",
    "    plt.scatter(x, y_wt[3456:8640, p], c='black', s=15, marker='+')\n",
    "    plt.scatter(weather_test['time'], weather_test[name], c='green', s=20)\n",
    "    plt.xlabel('Time (days)')\n",
    "    plt.ylabel(name)\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, y_wt.shape[1]*5))\n",
    "x = x_wt[3456:8640,0]\n",
    "for i, name in enumerate(weather_test.columns[1:]):\n",
    "    p = list(weather_train.columns).index(name)\n",
    "    plt.subplot(y_wt.shape[1],1,i+1)\n",
    "\n",
    "    \n",
    "    mean = np.mean(Hmeans[p],axis=0)\n",
    "    var = np.mean(Hmeans[p]**2 + Hvars[p],axis=0) - mean**2\n",
    "    mean, var = mean[:,0],var[:,0]\n",
    "    mean, std = mean* stds_weather[p] + means_weather[p], stds_weather[p] * np.sqrt(var)\n",
    "    \n",
    "    meanb = np.mean(BHmeans[p],axis=0)\n",
    "    varb = np.mean(BHmeans[p]**2 + BHvars[p],axis=0) - meanb**2\n",
    "    meanb, varb = meanb[:,0],varb[:,0]\n",
    "    meanb, stdb = meanb* stds_weather[p] + means_weather[p], stds_weather[p] * np.sqrt(varb)\n",
    "    \n",
    "    \n",
    "    plt.plot(x, mean, c='red', label = 'GPLAR')\n",
    "    plt.fill_between(x, mean-1.96*std, mean+1.96*std, facecolor='tab:red', alpha=.25)\n",
    "    \n",
    "    plt.plot(x, meanb, c='green', label = 'GPLAR')\n",
    "    plt.fill_between(x, meanb-1.96*stdb, meanb+1.96*stdb, facecolor='tab:green', alpha=.25)\n",
    "    \n",
    "    plt.scatter(x, y_wt[3456:8640, p], c='black', s=15, marker='+')\n",
    "    plt.scatter(weather_test['time'], weather_test[name], c='green', s=20)\n",
    "    plt.xlabel('Time (days)')\n",
    "    plt.ylabel(name)\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, y_wt.shape[1]*5))\n",
    "x = x_wt[3456:8640,0]\n",
    "for i, name in enumerate(weather_test.columns[1:]):\n",
    "    p = list(weather_train.columns).index(name)\n",
    "    plt.subplot(y_wt.shape[1],1,i+1)\n",
    "\n",
    "    \n",
    "    mean = np.mean(Hmeans[p],axis=0)\n",
    "    var = np.mean(Hmeans[p]**2 + Hvars[p],axis=0) - mean**2\n",
    "    mean, var = mean[:,0],var[:,0]\n",
    "    mean, std = mean* stds_weather[p] + means_weather[p], stds_weather[p] * np.sqrt(var)\n",
    "    \n",
    "    #meanb = np.mean(BHmeans[p],axis=0)\n",
    "    #varb = np.mean(BHmeans[p]**2 + BHvars[p],axis=0) - meanb**2\n",
    "    #meanb, varb = meanb[:,0],varb[:,0]\n",
    "    #meanb, stdb = meanb* stds_weather[p] + means_weather[p], stds_weather[p] * np.sqrt(varb)\n",
    "    \n",
    "    \n",
    "    plt.plot(x, mean, c='red', label = 'GPLAR')\n",
    "    plt.fill_between(x, mean-1.96*std, mean+1.96*std, facecolor='tab:red', alpha=.25)\n",
    "    \n",
    "    #plt.plot(x, meanb, c='green', label = 'GPLAR')\n",
    "    #plt.fill_between(x, meanb-1.96*stdb, meanb+1.96*stdb, facecolor='tab:green', alpha=.25)\n",
    "    \n",
    "    plt.scatter(x, y_wt[3456:8640, p], c='black', s=15, marker='+')\n",
    "    plt.scatter(weather_test['time'], weather_test[name], c='green', s=20)\n",
    "    plt.xlabel('Time (days)')\n",
    "    plt.ylabel(name)\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, y_wt.shape[1]*5))\n",
    "x = x_wt[3456:8640,0]\n",
    "for i, name in enumerate(weather_test.columns[1:]):\n",
    "    p = list(weather_train.columns).index(name)\n",
    "    plt.subplot(y_wt.shape[1],1,i+1)\n",
    "\n",
    "    \n",
    "    mean = np.mean(Hmeans[p],axis=0)\n",
    "    var = np.mean(Hmeans[p]**2 + Hvars[p],axis=0) - mean**2\n",
    "    mean, var = mean[:,0],var[:,0]\n",
    "    mean, std = mean* stds_weather[p] + means_weather[p], stds_weather[p] * np.sqrt(var)\n",
    "    \n",
    "    #meanb = np.mean(BHmeans[p],axis=0)\n",
    "    #varb = np.mean(BHmeans[p]**2 + BHvars[p],axis=0) - meanb**2\n",
    "    #meanb, varb = meanb[:,0],varb[:,0]\n",
    "    #meanb, stdb = meanb* stds_weather[p] + means_weather[p], stds_weather[p] * np.sqrt(varb)\n",
    "    \n",
    "    \n",
    "    plt.plot(x, mean, c='red', label = 'GPLAR')\n",
    "    plt.fill_between(x, mean-1.96*std, mean+1.96*std, facecolor='tab:red', alpha=.25)\n",
    "    \n",
    "    #plt.plot(x, meanb, c='green', label = 'GPLAR')\n",
    "    #plt.fill_between(x, meanb-1.96*stdb, meanb+1.96*stdb, facecolor='tab:green', alpha=.25)\n",
    "    \n",
    "    plt.scatter(x, y_wt[3456:8640, p], c='black', s=15, marker='+')\n",
    "    plt.scatter(weather_test['time'], weather_test[name], c='green', s=20)\n",
    "    plt.xlabel('Time (days)')\n",
    "    plt.ylabel(name)\n",
    "    plt.ylim(y_wt[~np.isnan(y_wt[:,p]), p].min()-5, y_wt[~np.isnan(y_wt[:,p]), p].max()+5)\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 5))\n",
    "x = x_wt[3456:8640,0]\n",
    "name = 'Bra temp'\n",
    "p = list(weather_train.columns).index(name)\n",
    "\n",
    "for i in range(len(Hmeans[p])):\n",
    "    mean = np.mean(Hmeans[p][i],axis=0)\n",
    "    var = np.mean(Hmeans[p][i]**2 + Hvars[p][i],axis=0) - mean**2\n",
    "    mean, var = mean[:,0],var[:,0]\n",
    "    mean, std = mean* stds_weather[p] + means_weather[p], stds_weather[p] * np.sqrt(var)\n",
    "    \n",
    "    if i==0: \n",
    "        plt.plot(x, mean, color='red', label='temproal')\n",
    "        plt.fill_between(x, mean-1.96*std, mean+1.96*std, facecolor='tab:red', alpha=.25)\n",
    "    else:\n",
    "        plt.plot(x, mean)\n",
    "        plt.fill_between(x, mean-1.96*std, mean+1.96*std, alpha=.25)\n",
    "    \n",
    " \n",
    "plt.scatter(x, y_wt[3456:8640, p], c='black', s=15, marker='+')\n",
    "plt.scatter(weather_test['time'], weather_test[name], c='green', s=20)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 5))\n",
    "x = x_wt[3456:8640,0]\n",
    "name = 'Bra temp'\n",
    "p = list(weather_train.columns).index(name)\n",
    "\n",
    "for i in range(len(Hmeans[p])):\n",
    "    mean = np.mean(Hmeans[p][i],axis=0)\n",
    "    var = np.mean(Hmeans[p][i]**2 + Hvars[p][i],axis=0) - mean**2\n",
    "    mean, var = mean[:,0],var[:,0]\n",
    "    mean, std = mean* stds_weather[p] + means_weather[p], stds_weather[p] * np.sqrt(var)\n",
    "    \n",
    "    if i==0: \n",
    "        plt.plot(x, mean, color='red', label='temproal')\n",
    "        plt.fill_between(x, mean-1.96*std, mean+1.96*std, facecolor='tab:red', alpha=.25)\n",
    "    else:\n",
    "        plt.plot(x, mean)\n",
    "        plt.fill_between(x, mean-1.96*std, mean+1.96*std, alpha=.25)\n",
    "    \n",
    " \n",
    "plt.scatter(x, y_wt[3456:8640, p], c='black', s=15, marker='+')\n",
    "plt.scatter(weather_test['time'], weather_test[name], c='green', s=20)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing temporal layers\n"
     ]
    }
   ],
   "source": [
    "M=300\n",
    "x_ind = np.linspace(x_wt.min(), x_wt.max(), M).reshape(-1,1)\n",
    "\n",
    "gplar_weather = GPLARmodel(x_wt, y_wt_normalise, M, 100, 1, x_ind, a, b, \n",
    "                    minibatch_size=1000,\n",
    "                    white=False,\n",
    "                    scale=0.2, per=True,\n",
    "                    linear=True, linear_scale=10.,\n",
    "                    nonlinear=True, nonlinear_scale=1.0,\n",
    "                    nonlinear_dependent=True,\n",
    "                    noise_inner=1e-5, noise_obs=1e-3,\n",
    "                    num_samples=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "gplar_weather.boosting_initialization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layers in gplar_weather.boosting_backwards_layers[1:]:\n",
    "    for layer in layers:\n",
    "        layer.q_sqrt = gpflow.base.Parameter(layer.q_sqrt*1e-3, transform=gpflow.utilities.triangular())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in gplar_weather.forwards_layers:\n",
    "    gpflow.set_trainable(layer.q_mu,False)\n",
    "    gpflow.set_trainable(layer.q_sqrt,False)\n",
    "for layer in gplar_weather.backwards_layers:\n",
    "    gpflow.set_trainable(layer.q_mu,False)\n",
    "    gpflow.set_trainable(layer.q_sqrt,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpflow.set_trainable(gplar_weather.inducing_locations, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gplar.boosting_initialization(y_wt_normalise,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gplar.boosting_backwards_initialization(y_wt_normalise,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layers in gplar.boosting_layers[1:]:\n",
    "    for layer in layers:\n",
    "        layer.q_sqrt = gpflow.base.Parameter(layer.q_sqrt.value() * 1e-5, \n",
    "                                 transform=gpflow.utilities.triangular())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "minibatch_size = 1000\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((x_wt, y_wt_normalise)).repeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gplar_weather.num_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float64, numpy=19911.515687312352>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = next(train_it)\n",
    "gplar_weather.maximum_log_likelihood_objective(*data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-14033762.65988556\n",
      "-11391172.258874895\n",
      "-7303718.278227894\n",
      "-8776952.911653325\n",
      "-7358457.206706711\n",
      "-3274676.349842146\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-0a7f739b1851>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mmaxiter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mci_niter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mrun_adam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgplar_weather\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mmaxiter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-43-0a7f739b1851>\u001b[0m in \u001b[0;36mrun_adam\u001b[0;34m(model, iterations)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mtrain_it\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mminibatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mneg_elbo\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0moptimization_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_it\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0melbo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mneg_elbo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/azureml_py36/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/azureml_py36/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    597\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/azureml_py36/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2361\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2363\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2365\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/azureml_py36/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1609\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1611\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1613\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/azureml_py36/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1690\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1692\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/azureml_py36/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/anaconda/envs/azureml_py36/lib/python3.6/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "@tf.function(autograph=False)\n",
    "def optimization_step(optimizer, model, data):\n",
    "    with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
    "        tape.watch(model.trainable_variables)\n",
    "        objective = -model.maximum_log_likelihood_objective(*data)\n",
    "        grads = tape.gradient(objective, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    return objective\n",
    "\n",
    "def run_adam(model, iterations):\n",
    "    learning_rate = 0.0001\n",
    "    adam = tf.optimizers.Adam(learning_rate=learning_rate)\n",
    "    train_it = iter(train_ds.batch(minibatch_size))\n",
    "    for step in range(iterations):\n",
    "        neg_elbo= optimization_step(adam, model, next(train_it))\n",
    "        elbo = -neg_elbo\n",
    "        if step%100 == 0:\n",
    "            print(elbo.numpy())\n",
    "\n",
    "maxiter = ci_niter(50000)\n",
    "run_adam(gplar_weather,  maxiter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def propagate(self, X, full_cov=False, S=1, zs=None):\n",
    "        \n",
    "    sX = tf.tile(tf.expand_dims(X, 0), [S, 1, 1]) # [S,N,1]\n",
    "    Hmeans, Hvars = [], []\n",
    "    zs = zs or [None, ] * len(self.temporal_layers) # [None, None, ..., None]\n",
    "    for temporal_layer, z, i in zip(self.temporal_layers, zs, range(self.num_outputs)):\n",
    "        Ht, Hmeant, Hvart = temporal_layer.sample_from_conditional(sX, z=z, full_cov=full_cov)\n",
    "        Hmeans.append([Hmeant,])\n",
    "        Hvars.append([Hvart,])\n",
    "        \n",
    "        for layers in self.boosting_layers[i+1:]:\n",
    "            Hf, Hmeanf, Hvarf = layers[i].sample_from_conditional(Ht, z=z, full_cov=full_cov)\n",
    "            Hmeans[i].append(Hmeanf)\n",
    "            Hvars[i].append(Hvarf)\n",
    "        for layers in self.boosting_backwards_layers[self.num_outputs-i:]:\n",
    "            Hb, Hmeanb, Hvarb = layers[self.num_outputs-i-1].sample_from_conditional(Ht, z=z, full_cov=full_cov)\n",
    "            Hmeans[i].append(Hmeanb)\n",
    "            Hvars[i].append(Hvarb)\n",
    "\n",
    "    return Hmeans, Hvars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hmeans, Hvars = propagate(gplar_weather,x_wt[3456:8640],S=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(Hmeans[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "@tf.function(autograph=False)\n",
    "def optimization_step(optimizer, model, data):\n",
    "    with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
    "        tape.watch(model.trainable_variables)\n",
    "        objective = -model.maximum_log_likelihood_objective(*data)\n",
    "        grads = tape.gradient(objective, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    return objective\n",
    "\n",
    "def run_adam(model, iterations):\n",
    "    adam = tf.optimizers.Adam(0.0001)\n",
    "    train_it = iter(train_ds.batch(minibatch_size))\n",
    "    for step in range(iterations):\n",
    "        neg_elbo= optimization_step(adam, model, next(train_it))\n",
    "        elbo = -neg_elbo\n",
    "        if step%500 == 0:\n",
    "            print(elbo.numpy())\n",
    "\n",
    "maxiter = ci_niter(50000)\n",
    "run_adam(gplar_weather,  maxiter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gpflow.utilities.print_summary(gplar_weather, fmt='notebook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hmean, Hvar = gplar.propagate(x_wt[3456:5184,:],S=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hmeans, Hvars = gplar_weather.propagate(x_wt[3456:8640,:],S=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hmean, Hvar = gplar.propagate(x_ex,S=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, y_wt.shape[1]*5))\n",
    "x = x_wt[3168:5472,0]\n",
    "for i, name in enumerate(['Bra speed','Sot speed','Cam speed']):\n",
    "    p = list(weather_train.columns).index(name)\n",
    "    plt.subplot(y_wt.shape[1],1,i+1)\n",
    "    mean = np.mean(Hmean[p],axis=0)\n",
    "    var = np.mean(Hmean[p]**2 + Hvar[p] + gplar.likelihoods[p].variance,axis=0) - mean**2\n",
    "    mean, var = mean[:,0],var[:,0]\n",
    "    mean, std = mean* stds_weather[p] + means_weather[p], stds_weather[p] * np.sqrt(var)\n",
    "    \n",
    "    plt.plot(x, mean, c='red', label = 'GPLAR')\n",
    "    plt.fill_between(x, mean-1.96*std, mean+1.96*std, facecolor='tab:red', alpha=.25)\n",
    "    \n",
    "    plt.scatter(x, y_wt[3168:5472, p], c='black', s=15, marker='+')\n",
    "    #plt.scatter(weather_test['time'], weather_test[name], c='green', s=20)\n",
    "    plt.xlabel('Time (second)')\n",
    "    plt.ylabel(name)\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, y_wt.shape[1]*5))\n",
    "x = x_wt[3456:5184,0]\n",
    "for i, name in enumerate(['Bra height','Sot height','Cam height']):\n",
    "    p = list(weather_train.columns).index(name)\n",
    "    plt.subplot(y_wt.shape[1],1,i+1)\n",
    "    mean = np.mean(Hmean[p],axis=0)\n",
    "    var = np.mean(Hmean[p]**2 + Hvar[p] + gplar.likelihoods[p].variance,axis=0) - mean**2\n",
    "    mean, var = mean[:,0],var[:,0]\n",
    "    mean, std = mean* stds_weather[p] + means_weather[p], stds_weather[p] * np.sqrt(var)\n",
    "    \n",
    "    plt.plot(x, mean, c='red', label = 'GPLAR')\n",
    "    plt.fill_between(x, mean-1.96*std, mean+1.96*std, facecolor='tab:red', alpha=.25)\n",
    "    \n",
    "    plt.scatter(x, y_wt[3456:5184, p], c='black', s=15, marker='+')\n",
    "    #plt.scatter(weather_test['time'], weather_test[name], c='green', s=20)\n",
    "    plt.xlabel('Time (second)')\n",
    "    plt.ylabel(name)\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, y_wt.shape[1]*5))\n",
    "x = x_wt[3168:5472,0]\n",
    "for i, name in enumerate(weather_test.columns[1:]):\n",
    "    p = list(weather_train.columns).index(name)\n",
    "    plt.subplot(y_wt.shape[1],1,i+1)\n",
    "    mean = np.mean((Hmeans[p]+BHmeans[p])/2.0,axis=0)\n",
    "    var = np.mean(((Hmeans[p]+BHmeans[p])/2.0)**2 + (Hvars[p]+BHvars[p])/4.0 + gplar.likelihoods[p].variance,axis=0) - mean**2\n",
    "    mean, var = mean[:,0],var[:,0]\n",
    "    mean, std = mean* stds_weather[p] + means_weather[p], stds_weather[p] * np.sqrt(var)\n",
    "    \n",
    "    plt.plot(x, mean, c='red', label = 'GPLAR')\n",
    "    plt.fill_between(x, mean-1.96*std, mean+1.96*std, facecolor='tab:red', alpha=.25)\n",
    "    \n",
    "    plt.scatter(x, y_wt[3168:5472, p], c='black', s=15, marker='+')\n",
    "    plt.scatter(weather_test['time'], weather_test[name], c='green', s=20)\n",
    "    plt.xlabel('Time (second)')\n",
    "    plt.ylabel(name)\n",
    "    plt.legend()\n",
    "    if name == 'Cam temp': plt.xlim(11.,17.)\n",
    "    if name == 'Chi temp': plt.xlim(13.,19.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, y_wt.shape[1]*5))\n",
    "x = x_wt[3456:8640,0]\n",
    "for i, name in enumerate(weather_test.columns[1:]):\n",
    "    p = list(weather_train.columns).index(name)\n",
    "    plt.subplot(y_wt.shape[1],1,i+1)\n",
    "    mean = np.mean((Hmeans[p]+BHmeans[p])/2.0,axis=0)\n",
    "    var = np.mean(((Hmeans[p]+BHmeans[p])/2.0)**2 + (Hvars[p]+BHvars[p])/4.0 + gplar.likelihoods[p].variance,axis=0) - mean**2\n",
    "    mean, var = mean[:,0],var[:,0]\n",
    "    mean, std = mean* stds_weather[p] + means_weather[p], stds_weather[p] * np.sqrt(var)\n",
    "    \n",
    "    plt.plot(x, mean, c='red', label = 'GPLAR')\n",
    "    plt.fill_between(x, mean-1.96*std, mean+1.96*std, facecolor='tab:red', alpha=.25)\n",
    "    \n",
    "    plt.scatter(x, y_wt[3456:8640, p], c='black', s=15, marker='+')\n",
    "    plt.scatter(weather_test['time'], weather_test[name], c='green', s=20)\n",
    "    plt.xlabel('Time (days)')\n",
    "    plt.ylabel(name)\n",
    "    plt.legend()\n",
    "    if name == 'Cam temp': plt.xlim(11.,17.)\n",
    "    if name == 'Chi temp': plt.xlim(13.,19.)\n",
    "    if name == 'Bra temp': plt.xlim(23.,29.)\n",
    "    if name == 'Sot temp': plt.xlim(25.,31.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, y_wt.shape[1]*5))\n",
    "x = x_wt[3168:5472,0]\n",
    "for i, name in enumerate(weather_test.columns[1:]):\n",
    "    p = list(weather_train.columns).index(name)\n",
    "    plt.subplot(y_wt.shape[1],1,i+1)\n",
    "    mean = np.mean(Hmean[p],axis=0)\n",
    "    var = np.mean(Hmean[p]**2 + Hvar[p] + gplar.likelihoods[p].variance,axis=0) - mean**2\n",
    "    mean, var = mean[:,0],var[:,0]\n",
    "    mean, std = mean* stds_weather[p] + means_weather[p], stds_weather[p] * np.sqrt(var)\n",
    "    \n",
    "    plt.plot(x, mean, c='red', label = 'GPLAR')\n",
    "    plt.fill_between(x, mean-1.96*std, mean+1.96*std, facecolor='tab:red', alpha=.25)\n",
    "    \n",
    "    plt.scatter(x, y_wt[3168:5472, p], c='black', s=15, marker='+')\n",
    "    plt.scatter(weather_test['time'], weather_test[name], c='green', s=20)\n",
    "    plt.xlabel('Time (second)')\n",
    "    plt.ylabel(name)\n",
    "    plt.legend()\n",
    "    if name == 'Cam temp': plt.xlim(11.,17.)\n",
    "    if name == 'Chi temp': plt.xlim(13.,19.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, y_wt.shape[1]*5))\n",
    "x = x_wt[3168:5472,0]\n",
    "for i, name in enumerate(weather_test.columns[1:]):\n",
    "    p = list(weather_train.columns).index(name)\n",
    "    plt.subplot(y_wt.shape[1],1,i+1)\n",
    "    mean = np.mean(Hmean[p],axis=0)\n",
    "    var = np.mean(Hmean[p]**2 + Hvar[p] + gplar.likelihoods[p].variance,axis=0) - mean**2\n",
    "    mean, var = mean[:,0],var[:,0]\n",
    "    mean, std = mean* stds_weather[p] + means_weather[p], stds_weather[p] * np.sqrt(var)\n",
    "    \n",
    "    plt.plot(x, mean, c='red', label = 'GPLAR')\n",
    "    plt.fill_between(x, mean-1.96*std, mean+1.96*std, facecolor='tab:red', alpha=.25)\n",
    "    \n",
    "    plt.scatter(x, y_wt[3168:5472, p], c='black', s=15, marker='+')\n",
    "    plt.scatter(weather_test['time'], weather_test[name], c='green', s=20)\n",
    "    plt.xlabel('Time (second)')\n",
    "    plt.ylabel(name)\n",
    "    plt.legend()\n",
    "    if name == 'Cam temp': plt.xlim(11.,17.)\n",
    "    if name == 'Chi temp': plt.xlim(13.,19.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_log_density(Hmean, Hvar, train, test, gplar):\n",
    "    log_density = []\n",
    "    for i, name in enumerate(test.columns[1:]):\n",
    "        Ynew = np.array(test[name])\n",
    "        p = list(train.columns).index(name)\n",
    "        mean = Hmean[p].numpy()\n",
    "        variance = Hvar[p].numpy()\n",
    "        mu = mean[:,~np.isnan(Ynew),0] * stds_weather[p] + means_weather[p] #[S,N]\n",
    "        var = variance[:,~np.isnan(Ynew),0] * stds_weather[p]**2 + gplar.likelihoods[p].variance #[S,N]\n",
    "        l = -0.5*(np.log(2*np.pi)+np.log(var)+np.square(Ynew[~np.isnan(Ynew)] - mu)/var) #[S,N]\n",
    "        log_num_samples = tf.math.log(tf.cast(mean.shape[0], l.dtype))\n",
    "        log_likelihood = tf.reduce_sum(tf.reduce_logsumexp(l - log_num_samples, axis=0)) #[N,] -> scalar\n",
    "        log_density.append([name, log_likelihood.numpy()])\n",
    "    return log_density\n",
    "\n",
    "def gplar_smse(Hmean, Hvar, train, test):\n",
    "    smse = []\n",
    "    for i, name in enumerate(test.columns[1:]):\n",
    "        Ynew = np.array(test[name])\n",
    "        p = list(train.columns).index(name)\n",
    "        mean = Hmean[p].numpy()\n",
    "        mu = mean[:,~np.isnan(Ynew),0] * stds_weather[p] + means_weather[p]\n",
    "        smse_i = np.mean((mu-Ynew[~np.isnan(Ynew)])**2)/np.mean((np.mean(Ynew[~np.isnan(Ynew)])-Ynew[~np.isnan(Ynew)])**2)\n",
    "        smse.append([name, smse_i])\n",
    "    return smse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_log_density_gpar(train, test, samples, reverse=False):\n",
    "    means, vars = np.mean(samples,axis=0), np.var(samples,axis=0)\n",
    "    log_density = []\n",
    "    for i, name in enumerate(test.columns[1:]):\n",
    "        y = np.array(test[name])\n",
    "        if reverse: p = means.shape[1]-list(train.columns).index(name)-1\n",
    "        else: p = list(train.columns).index(name)\n",
    "        Y = y[~np.isnan(y)]\n",
    "        mu, var = means[~np.isnan(y),p], vars[~np.isnan(y),p]\n",
    "        log_likelihood = -0.5*(np.log(2*np.pi)+np.log(var)+np.square(Y - mu)/var)\n",
    "        log_density.append([name,np.sum(log_likelihood)])\n",
    "    return log_density\n",
    "\n",
    "def gpar_smse(train, test, samples, reverse=False):\n",
    "    smse=[]\n",
    "    means = np.mean(samples, axis=0)\n",
    "    for i, name in enumerate(test.columns[1:]):\n",
    "        y = np.array(test[name])\n",
    "        if reverse: p = means.shape[1]-list(train.columns).index(name)-1\n",
    "        else: p = list(train.columns).index(name)\n",
    "        mu = means[~np.isnan(y),p]\n",
    "        smse_i = np.mean((mu-y[~np.isnan(y)])**2)/np.mean((np.mean(y[~np.isnan(y)])-y[~np.isnan(y)])**2)\n",
    "        smse.append([name, smse_i])\n",
    "    return smse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cam temp and Chi temp are the last two output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_log_density(Hmean, Hvar, weather_train, weather_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gplar_smse(Hmean, Hvar,  weather_train, weather_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cam temp and Chi temp are the first two output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_log_density(Hmean, Hvar, weather_train, weather_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gplar_smse(Hmean, Hvar,  weather_train, weather_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bi-directional GPLAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_log_density(BHmeans, BHvars, weather_train, weather_test, gplar_weather)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gplar_smse(BHmeans, BHvars,  weather_train, weather_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_log_density_gpar(weather_train, weather_test, backward_samples_, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpar_smse(weather_train, weather_test, backward_samples_, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gplar_smse(Hmeans, Hvars,  weather_train, weather_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_log_density(Hmeans, Hvars, weather_train, weather_test, gplar_weather)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_log_density_gpar(weather_train, weather_test, forward_samples_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpar_smse(weather_train, weather_test, forward_samples_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, y_ex.shape[1]*5))\n",
    "x = x_ex[:,0]\n",
    "for i, name in enumerate(exchange_train_1.columns):\n",
    "    plt.subplot(y_ex.shape[1],1,i+1)\n",
    "    mean = np.mean(Hmean[i],axis=0)\n",
    "    var = np.mean(Hmean[i]**2 + Hvar[i] + gplar.likelihoods[i].variance,axis=0) - mean**2\n",
    "    mean, var = mean[:,0],var[:,0]\n",
    "    mean, std = mean* stds_exchange[i] + means_exchange[i], stds_exchange[i] * np.sqrt(var)\n",
    "    \n",
    "    plt.plot(x, mean, c='red', label = 'GPLAR')\n",
    "    plt.fill_between(x, mean-1.96*std, mean+1.96*std, facecolor='tab:red', alpha=.25)\n",
    "    \n",
    "    plt.scatter(x, y_ex[:, i], c='black', s=15, marker='+')\n",
    "    if name in exchange_test_1.columns:\n",
    "        plt.scatter(exchange_test_1['year'], exchange_test_1[name], c='green', s=20)\n",
    "    plt.xlabel('Time (second)')\n",
    "    plt.ylabel(f'{name} (volt)')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, y_eeg.shape[1]*5))\n",
    "x = x_eeg[:,0]\n",
    "for i, name in enumerate(eeg_train_2.columns):\n",
    "    plt.subplot(y_eeg.shape[1],1,i+1)\n",
    "    \n",
    "    mean = np.mean((Hmeans[i] + BHmeans[i])/2.,axis=0)\n",
    "    meanf = np.mean(Hmeans[i], axis=0)\n",
    "    varf = np.mean(Hmeans[i]**2+Hvars[i], axis=0)-meanf**2\n",
    "    meanb = np.mean(BHmeans[i],axis=0)\n",
    "    varb = np.mean(BHmeans[i]**2+BHvars[i], axis=0)-meanb**2\n",
    "    var = np.mean(((Hmeans[i]+BHmeans[i])/2.0)**2 + (Hvars[i]+BHvars[i])/4.0 + gplar.likelihoods[i].variance,axis=0) - mean**2\n",
    "    mean, var = mean[:,0],var[:,0]\n",
    "    mean, std = mean* stds_eeg[i] + means_eeg[i], stds_eeg[i] * np.sqrt(var)\n",
    "    meanf, meanb = meanf[:,0]*stds_eeg[i]+means_eeg[i], meanb[:,0]*stds_eeg[i]+means_eeg[i]\n",
    "    stdf, stdb = stds_eeg[i]*np.sqrt(varf[:,0]), stds_eeg[i]*np.sqrt(varb[:,0])\n",
    "    \n",
    "    plt.plot(x, mean, c='red', label = 'GPLAR')\n",
    "    plt.plot(x, meanf, c='blue', label='forward')\n",
    "    plt.fill_between(x, meanf-1.96*stdf, meanf+1.96*stdf, facecolor='tab:blue', alpha=.25)\n",
    "    plt.plot(x, meanb, c='green', label='backward')\n",
    "    plt.fill_between(x, meanb-1.96*stdb, meanb+1.96*stdb, facecolor='tab:green', alpha=.25)\n",
    "    \n",
    "    plt.scatter(x, y_eeg[:, i], c='black', s=15, marker='+')\n",
    "    if name in eeg_test_2.columns:\n",
    "        plt.scatter(eeg_test_2['time'], eeg_test_2[name], c='green', s=20)\n",
    "    plt.xlabel('Time (second)')\n",
    "    plt.xlim(0, 1)\n",
    "    plt.ylabel(f'{name} (volt)')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('pascal-weather-NL-L-path2-reverse.txt','w+')\n",
    "f.write('nonlinear:'+'\\n') \n",
    "for layers in gplar.boosting_layers[1:]:\n",
    "    current = ''\n",
    "    for layer in layers:\n",
    "        current += str(layer.kernel.kernels[0].variance.value().numpy()) + ' '\n",
    "    f.write(current + '\\n')\n",
    "f.write('back:'+'\\n') \n",
    "for layers in gplar.boosting_backwards_layers[1:]:\n",
    "    current = ''\n",
    "    for layer in layers:\n",
    "        current += str(layer.kernel.kernels[0].variance.value().numpy()) + ' '\n",
    "    f.write(current + '\\n')\n",
    "f.write('linear:'+'\\n') \n",
    "for layers in gplar.boosting_layers[1:]:\n",
    "    current = ''\n",
    "    for layer in layers:\n",
    "        current += str(layer.kernel.kernels[1].variance.value().numpy()) + ' '\n",
    "    f.write(current + '\\n')\n",
    "f.write('back:'+'\\n') \n",
    "for layers in gplar.boosting_backwards_layers[1:]:\n",
    "    current = ''\n",
    "    for layer in layers:\n",
    "        current += str(layer.kernel.kernels[1].variance.value().numpy()) + ' '\n",
    "    f.write(current + '\\n')\n",
    "f.write('temporal:'+'\\n') \n",
    "for layer in gplar.temporal_layers:\n",
    "    f.write(str(layer.kernel.kernels[1].variance.value().numpy())+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3-azureml"
  },
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
